{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y95EwaJaWj16"
      },
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyBbRMYKPvu4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGs9gXH3ZjgA",
        "outputId": "ad5309fe-d7fe-4d73-d99a-1132e5129640"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"dev.zip\"):\n",
        "  !wget \"https://huggingface.co/datasets/gabrielrstan/CORAA-v1.1/resolve/main/dev.zip\" -P \".\"\n",
        "\n",
        "if not os.path.isdir(\"dev/\"):\n",
        "  !unzip \"dev.zip\" -d \".\"\n",
        "\n",
        "if os.path.isdir(\"dev/\"):\n",
        "  clear_output()\n",
        "  print(\"data unzipped\")\n",
        "else:\n",
        "  # !rm /content/dev.zip\n",
        "  raise Exception(\"Cannot unzip.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhZavogveUkh",
        "outputId": "08e5787e-c252-4611-dfcf-291865667d1f"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"metadata_dev_final.csv\"):\n",
        "  !wget \"https://huggingface.co/datasets/gabrielrstan/CORAA-v1.1/resolve/main/metadata_dev_final.csv\" -P \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWJ9kzAfRGw7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "L4uqDEjqeY3J",
        "outputId": "ea85da8b-0e36-451c-c9f4-260494dcdac1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('metadata_dev_final.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "-UCLjH5sFT_5",
        "outputId": "c0b8fa0a-aef4-4615-f94f-03d15bdc7be7"
      },
      "outputs": [],
      "source": [
        "df[['up_votes', 'down_votes']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoAhWB3YWqLq"
      },
      "source": [
        "## manipulate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "qbxqThbGGI50",
        "outputId": "202fc02c-5adf-4e05-9229-9ebccd2686e6"
      },
      "outputs": [],
      "source": [
        "temp_df = df.copy()\n",
        "temp_df = temp_df[temp_df['up_votes'] > 0]\n",
        "temp_df = temp_df[temp_df['down_votes'] == 0]\n",
        "temp_df = temp_df.reset_index(drop=True)\n",
        "temp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IM_cPUu4e_iE",
        "outputId": "38165ded-19c5-437b-89a7-a0669a1c6fcd"
      },
      "outputs": [],
      "source": [
        "df_hesitation = temp_df[['file_path','votes_for_hesitation']].dropna()\n",
        "df_hesitation['has_hesitation'] = (df_hesitation['votes_for_hesitation'] > 0).astype(int)\n",
        "df_hesitation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnsu9SKmfwEN"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "# remove audios with min_limite length\n",
        "\n",
        "MINIMUM_DURATION = 5 # seconds\n",
        "\n",
        "def audiofile_duration(file_path: str) -> float:\n",
        "  waveform, sample_rate = librosa.load(file_path)\n",
        "  duration = librosa.get_duration(y=waveform, sr=sample_rate)\n",
        "  return duration\n",
        "\n",
        "def get_df_with_minimum_duration(df:pd.DataFrame, minimum_duration:float) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  df: dataframe containing 'file_path'\n",
        "  minimum_duration: minimum audio duration\n",
        "  \"\"\"\n",
        "  df['audio_duration'] = df['file_path'].map(audiofile_duration)\n",
        "  df = df[df['audio_duration'] > minimum_duration]\n",
        "  return df.reset_index(drop=True)\n",
        "\n",
        "df_hesitation = get_df_with_minimum_duration(df_hesitation, MINIMUM_DURATION)\n",
        "df_hesitation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCQMIjXjDID5"
      },
      "outputs": [],
      "source": [
        "df_hesitation['has_hesitation'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DlsPmTLDf35"
      },
      "outputs": [],
      "source": [
        "from random import sample\n",
        "\n",
        "has_hesitation_count = df_hesitation['has_hesitation'].value_counts()[1]\n",
        "not_has_hesitation_count = df_hesitation['has_hesitation'].value_counts()[0]\n",
        "to_remove = sample(list(df_hesitation[df_hesitation['has_hesitation'] == 0].index), not_has_hesitation_count-has_hesitation_count)\n",
        "df_hesitation = df_hesitation.drop(to_remove).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rutvFMojAEZC"
      },
      "outputs": [],
      "source": [
        "df_hesitation['has_hesitation'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_hesitation['audio_duration'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFU1a0_hW8ml"
      },
      "source": [
        "## setting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xws5N26IRTGE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
        "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNy07RYwRYW2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, padding_side=\"right\", sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
        "# processor = AutoProcessor.from_pretrained(\"alefiury/wav2vec2-xls-r-300m-pt-br-spontaneous-speech-emotion-recognition\")\n",
        "\n",
        "def get_model(dropout: float):\n",
        "  # model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "  model = AutoModelForAudioClassification.from_pretrained(\"alefiury/wav2vec2-xls-r-300m-pt-br-spontaneous-speech-emotion-recognition\")\n",
        "\n",
        "  # MIT/ast-finetuned-audioset-10-10-0.4593\n",
        "  # dense_in_features = model.classifier.dense.in_features\n",
        "  # n_classes = 2\n",
        "  # model.classifier.dense = nn.Sequential(\n",
        "  #     nn.Dropout(dropout, inplace=True),\n",
        "  #     nn.Linear(in_features=dense_in_features, out_features=n_classes),\n",
        "  # )\n",
        "\n",
        "\n",
        "  # alefiury/wav2vec2-xls-r-300m-pt-br-spontaneous-speech-emotion-recognition\n",
        "  dense_in_features = model.classifier.in_features\n",
        "  n_classes = 2\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(dropout, inplace=True),\n",
        "      nn.Linear(in_features=dense_in_features, out_features=n_classes),\n",
        "  )\n",
        "  model\n",
        "\n",
        "  \n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  for param in model.classifier.parameters():\n",
        "      param.requires_grad = True\n",
        "      \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLQUnCPDXEqK"
      },
      "source": [
        "## setting dataset to torch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoDKbcpXXxOi"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def get_features(file_paths:list) -> torch.Tensor:\n",
        "  sampling_rate = 16_000\n",
        "  waveforms = []\n",
        "  for file_path in file_paths:\n",
        "    waveform, original_samplerate = librosa.load(file_path)\n",
        "    waveform = librosa.resample(waveform, orig_sr=original_samplerate, target_sr=sampling_rate)\n",
        "    waveforms.append(waveform)\n",
        "\n",
        "  # MIT/ast-finetuned-audioset-10-10-0.4593\n",
        "  #features = feature_extractor(waveforms, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "  # alefiury/wav2vec2-xls-r-300m-pt-br-spontaneous-speech-emotion-recognition\n",
        "  features = feature_extractor(waveforms, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=10000)\n",
        "  \n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_xmWa_kbOCn"
      },
      "outputs": [],
      "source": [
        "features = get_features(df_hesitation['file_path'])\n",
        "features['input_values'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nBzFZmLeyfO"
      },
      "outputs": [],
      "source": [
        "features['labels'] = torch.tensor(df_hesitation['has_hesitation'],dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QUOoYnFzYgg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features['input_values'], features['labels'], test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApmVCJR2fKKs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9zAHSC2XGsx"
      },
      "source": [
        "## training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7dbFEOB2ziy"
      },
      "outputs": [],
      "source": [
        "def eval_model(model) -> tuple:\n",
        "  \"\"\"\n",
        "  return:\n",
        "  loss\n",
        "  accuracy\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0\n",
        "  running_corrects = 0\n",
        "  for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs).logits\n",
        "      loss = cross_entropy(logits, labels)\n",
        "      predicted_class = torch.argmax(logits, dim=-1)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      running_corrects += sum(labels == predicted_class)\n",
        "  loss = (running_loss/len(test_dataloader))\n",
        "  accuracy = running_corrects/(len(test_dataloader) * test_dataloader.batch_size)\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "put_train = ray.put(train_dataloader)\n",
        "put_test = ray.put(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIyIGyxM56Uq"
      },
      "outputs": [],
      "source": [
        "def train_func(model, dataloader, optimizer, exp_lr_scheduler, clip_value):\n",
        "  total = 0\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.train()\n",
        "  running_loss = 0\n",
        "  correct = 0\n",
        "  for i, (data, target) in enumerate(dataloader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(data).logits\n",
        "    loss = cross_entropy(output, target)\n",
        "\n",
        "    total += output.size(0)\n",
        "    running_loss += loss.item() * output.size(0)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), clip_value)\n",
        "    optimizer.step()\n",
        "        # accuracy\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    # _, correct_class = torch.max(target.data, 1)\n",
        "\n",
        "    correct += (predicted == target).sum().item()\n",
        "\n",
        "  exp_lr_scheduler.step()\n",
        "\n",
        "  return {\n",
        "      \"mean_loss\": running_loss / total,\n",
        "      \"mean_accuracy\": correct / total,\n",
        "  }\n",
        "\n",
        "def test_func(model, dataloader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data).logits\n",
        "\n",
        "            # accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # _, correct_class = torch.max(target.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            # loss\n",
        "            running_loss += cross_entropy(outputs, target).item() * outputs.size(0)\n",
        "\n",
        "    return {\n",
        "        \"mean_loss\": running_loss / total,\n",
        "        \"mean_accuracy\": correct / total,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2eFlco56ypx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "from ray import train\n",
        "from ray.train import Checkpoint\n",
        "\n",
        "def train_hesitation(config, max_epochs=30, tunning=True):\n",
        "    train_dataloader = ray.get(put_train)\n",
        "    test_dataloader = ray.get(put_test)\n",
        "\n",
        "    # Data Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = get_model(config['classifier_dropout'])\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        model.classifier.parameters(),\n",
        "        lr=config[\"lr\"],\n",
        "        momentum=config[\"momentum\"],\n",
        "        weight_decay=config['weight_decay'],\n",
        "        nesterov=config['nesterov']\n",
        "    )\n",
        "\n",
        "    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=config['lr_scheduler_gamma'])\n",
        "    for i in range(max_epochs):\n",
        "        train_log = train_func(model, train_dataloader, optimizer, exp_lr_scheduler, config['clip_value'])\n",
        "        val_log = test_func(model, test_dataloader)\n",
        "\n",
        "        if tunning:\n",
        "            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
        "                checkpoint = None\n",
        "                if (i + 1) % max_epochs == 0 and (val_log[\"mean_loss\"] < 0.4):\n",
        "                    # This saves the model to the trial directory\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(temp_checkpoint_dir, \"model.pth\")\n",
        "                    )\n",
        "                    checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
        "\n",
        "                # Send the current training result back to Tune\n",
        "                train.report(\n",
        "                    {\n",
        "                        \"train_mean_loss\": train_log[\"mean_loss\"],\n",
        "                        \"train_mean_accuracy\": train_log[\"mean_accuracy\"],\n",
        "                        \"val_mean_loss\": val_log[\"mean_loss\"],\n",
        "                        \"val_mean_accuracy\": val_log[\"mean_accuracy\"],\n",
        "                    },\n",
        "                    checkpoint=checkpoint\n",
        "                )\n",
        "        else:\n",
        "            print(\"-\"*10, f\"epoch: {i+1}/{max_epochs}\",\"-\"*10)\n",
        "            print(f\"train: {train_log}\\nval: {val_log}\")\n",
        "    if not tunning:\n",
        "        return {\n",
        "            \"model\": model,\n",
        "            \"log\": {\n",
        "                \"train\": train_log,\n",
        "                \"val\": val_log,\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_cpus = os.cpu_count()\n",
        "max_gpus = torch.cuda.device_count()\n",
        "max_cpus, max_gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"lr\": tune.loguniform(1e-5, 1e-0),\n",
        "    \"momentum\": tune.uniform(0.1, 0.9),\n",
        "    \"classifier_dropout\": tune.uniform(0.3, 0.7),\n",
        "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
        "    \"clip_value\": tune.randint(1, 5+1),\n",
        "    \"lr_scheduler_gamma\": tune.uniform(0.5, 1.0),\n",
        "    #\"image_net\": tune.choice([\"IMAGENET1K_V1\", \"IMAGENET1K_V2\"]),\n",
        "    \"nesterov\": tune.choice([True, False]),\n",
        "    #\"transfer_model\": tune.choice([\"resnet152\", \"mobiletenet_v3_large\"])\n",
        "}\n",
        "\n",
        "metric = \"val_mean_loss\"\n",
        "mode = \"min\"\n",
        "\n",
        "optuna_search = OptunaSearch(\n",
        "    metric=metric,\n",
        "    mode=mode,\n",
        "    # points_to_evaluate = curr_best_params,\n",
        ")\n",
        "\n",
        "asas_scheduler = ASHAScheduler(\n",
        "    time_attr='training_iteration',\n",
        "    metric=metric,\n",
        "    mode=mode,\n",
        "    max_t=5,\n",
        "    grace_period=1,\n",
        "    reduction_factor=3,\n",
        "    brackets=2\n",
        ")\n",
        "\n",
        "trainable_with_resources = tune.with_resources(train_hesitation, {\"cpu\": max_cpus, \"gpu\": max_gpus})\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    trainable_with_resources,\n",
        "    tune_config=tune.TuneConfig(\n",
        "        num_samples=100,\n",
        "        search_alg=optuna_search,\n",
        "        scheduler=asas_scheduler\n",
        "    ),\n",
        "    param_space=config,\n",
        ")\n",
        "results = tuner.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.get_dataframe().to_csv(\"raytune_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_results = results.get_dataframe()\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "scores = f1_score(df_results['val_mean_accuracy'].map.plot(kind='hist', title='val_mean_accuracy hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_result = results.get_best_result(\"val_mean_accuracy\", mode=\"max\")\n",
        "best_result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_result = results.get_best_result(\"val_mean_loss\", mode=\"min\")\n",
        "best_result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"best_result.json\", 'w') as f:\n",
        "    json.dump(best_result.config, f, default=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_config_train_model = train_hesitation(best_result.config, max_epochs=8, tunning=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nukqKS2AXKqj"
      },
      "source": [
        "## evaluating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = best_config_train_model['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aicm9Xz_9e6r"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  with torch.no_grad():\n",
        "    logits = model(inputs).logits\n",
        "    predicted_class_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "  all_labels.append(labels)\n",
        "  all_preds.append(predicted_class_ids)\n",
        "\n",
        "all_labels = torch.concat(all_labels).cpu()\n",
        "all_preds = torch.concat(all_preds).cpu()\n",
        "\n",
        "all_labels.shape, all_preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IxUchOXECpw"
      },
      "outputs": [],
      "source": [
        "classes_names = ['ausent', 'hesitation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H88ihItxCxY4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BttAIybDbNL"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes_names)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=classes_names))\n",
        "print(\"\\naccuracy:\", accuracy_score(all_labels, all_preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
